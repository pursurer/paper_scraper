# Paper Scraper - 顶会论文获取工具

## 项目简介
一个用于从多个来源批量获取 AI 顶会论文元数据的 Python 工具。支持 ICML、ICLR、NeurIPS、AAAI、IJCAI、ACL、EMNLP 等主流 AI 会议，可通过关键词过滤或全量抓取方式获取论文的标题、摘要、关键词、PDF 链接等信息，并导出为 CSV/PKL 格式。

## 数据来源

| 来源类型 | 支持会议 | 获取方式 | 参考实现 |
|---------|---------|---------|---------|
| **OpenReview API** | ICLR, ICML, NeurIPS | 直接调用 API 获取元数据 | `Openreview论文获取方案/` |
| **网页爬取** | AAAI, IJCAI, ACL, EMNLP, NAACL, AISTATS | 解析官网 HTML 获取论文列表 | `AAMAS论文获取/paper_downloader/code/paper_downloader_*.py` |
| **PDF 提取** | AAMAS | 下载 PDF 后提取 title/abstract/keywords | `AAMAS论文获取/paper_downloader/code/extract_aamas_metadata.py` |

## 代码库指南 (Repo Orientation)

### 核心模块
| 文件 | 职责 |
|------|------|
| `scraper.py` | **主入口类** - `Scraper` 类，协调 OpenReview 抓取流程 |
| `paper.py` | **论文获取** - 从 OpenReview API 获取论文数据 |
| `venue.py` | **会议发现** - 获取和分组 OpenReview venues |
| `extractor.py` | **字段提取** - `Extractor` 类，从论文对象中提取指定字段 |
| `filters.py` | **关键词过滤** - 基于模糊匹配的标题/摘要/关键词过滤器 |
| `web_scraper.py` | **网页爬取** - AAAI/IJCAI/ACL 等会议的 HTML 解析 |
| `pdf_extractor.py` | **PDF 提取** - 从 PDF 中提取元数据（AAMAS） |
| `utils.py` | **工具函数** - API 客户端、CSV 导出、重试机制等 |
| `config.py` | **配置文件** - OpenReview 账号凭证 |

### 使用脚本
| 文件 | 用途 |
|------|------|
| `scrape.py` | 统一 CLI 入口，支持所有会议 |
| `batch_scrape.py` | 批量抓取多个会议 |

## 文件结构 (File Structure)

```
论文获取/
├── paper_scraper/              # 核心 Python 包
│   ├── __init__.py             # 包入口，定义数据来源
│   │
│   ├── # OpenReview 来源
│   ├── scraper.py              # Scraper 主类
│   ├── paper.py                # 论文获取逻辑
│   ├── venue.py                # Venue 处理
│   │
│   ├── # 网页爬取来源
│   ├── web_scraper.py          # HTML 解析（AAAI/IJCAI/ACL等）
│   │
│   ├── # PDF 提取来源
│   ├── pdf_extractor.py        # PDF 元数据提取（AAMAS）
│   │
│   ├── # 通用模块
│   ├── extractor.py            # 字段提取器
│   ├── filters.py              # 关键词过滤器
│   └── utils.py                # 通用工具函数
│
├── scripts/                    # 使用脚本
│   ├── scrape.py               # 统一 CLI 入口
│   └── batch_scrape.py         # 批量抓取
│
├── tests/                      # 测试文件
│   ├── __init__.py
│   ├── test_package.py
│   ├── test_utils.py
│   └── ...
│
├── config/                     # 配置目录
│   ├── __init__.py
│   └── config.example.py       # 配置模板
│
├── 文件结构.md                  # 本文件
├── 项目指南.md                  # TDD 行动指南
├── AGENTS.md                   # AI Agent 开发指南
├── .gitignore                  # Git 忽略规则
├── requirements.txt            # Python 依赖
└── README.md                   # 项目说明
```

## 设置与环境 (Setup and Environment)

### 依赖安装
```bash
# 进入项目目录
cd 论文获取

# 创建虚拟环境
python -m venv venv
source venv/bin/activate  # macOS/Linux
# venv\Scripts\activate   # Windows

# 安装依赖
pip install -r requirements.txt
```

### 配置凭证（仅 OpenReview 来源需要）
```bash
cp config/config.example.py config/config.py
# 编辑 config.py，填入你的 OpenReview 账号
```

```python
# config/config.py
EMAIL = "your_email@example.com"
PASSWORD = "your_password"
```

### 依赖项 (requirements.txt)
```
# OpenReview 来源
openreview-py>=0.6.0

# 网页爬取来源
beautifulsoup4>=4.9.0
requests>=2.25.0
html5lib>=1.1

# PDF 提取来源
PyMuPDF>=1.18.0

# 通用
dill>=0.3.0
thefuzz>=0.20.0
pytest>=7.0.0
```

## 最佳实践 (Best Practices)

### 1. API 速率限制处理
- OpenReview 内置指数退避重试机制（最多 5 次重试）
- 网页爬取时建议请求间隔 3-5 秒
- 遇到 429 错误会自动等待并重试

### 2. 数据去重
- 基于 `forum` 或 `title+year` 自动去重
- CSV 追加模式会自动合并去重

### 3. 推荐工作流
```
1. 确定会议和年份
2. 选择对应的数据来源
3. 运行抓取脚本
4. 检查输出 CSV
5. 如有失败，运行重试脚本
```

### 4. 数据备份
- 同时保存 CSV 和 PKL 格式
- PKL 包含原始对象，便于后续处理

## 代码风格 (Code Style)

- **语言**: Python 3.8+
- **命名规范**: 
  - 函数/变量: `snake_case`
  - 类名: `PascalCase`
  - 常量: `UPPER_CASE`
- **文档**: 中英文混合注释
- **错误处理**: 使用 try-except 并打印友好的状态信息（✅ ❌ ⚠️）

## 访问模式 (Access Patterns)

### OpenReview 来源 (ICLR, ICML, NeurIPS)
```python
from paper_scraper import Scraper, Extractor
from paper_scraper.filters import title_filter, abstract_filter

extractor = Extractor(
    fields=['forum'],
    subfields={'content': ['title', 'keywords', 'abstract', 'pdf']}
)

scraper = Scraper(
    conferences=['ICLR'],
    years=['2024'],
    keywords=['generalization'],
    extractor=extractor,
    fpath='output.csv',
    only_accepted=True
)

scraper.add_filter(title_filter)
scraper()
```

### 网页爬取来源 (AAAI, IJCAI 等)
```python
from paper_scraper.web_scraper import scrape_aaai, scrape_ijcai

# 抓取 AAAI 2025
papers = scrape_aaai(year=2025, output_path='aaai_2025.csv')

# 抓取 IJCAI 2024
papers = scrape_ijcai(year=2024, output_path='ijcai_2024.csv')
```

### PDF 提取来源 (AAMAS)
```python
from paper_scraper.pdf_extractor import extract_aamas_metadata

# 从 PDF 目录提取元数据
papers = extract_aamas_metadata(
    pdf_dir='./aamas2025/',
    output_path='aamas_2025.csv'
)
```

### 命令行使用
```bash
# OpenReview 来源
python scripts/scrape.py --conference ICLR --years 2024

# 网页爬取来源
python scripts/scrape.py --conference AAAI --years 2025

# 批量抓取
python scripts/batch_scrape.py --conferences ICLR ICML NeurIPS --years 2024
```

## API 与契约 (APIs and Contracts)

### Scraper 类 (OpenReview)
```python
class Scraper:
    def __init__(
        self,
        conferences: List[str],      # 会议名称列表 ['ICLR', 'ICML']
        years: List[str],            # 年份列表 ['2024']
        keywords: List[str],         # 过滤关键词
        extractor: Extractor,        # 字段提取器
        fpath: str,                  # 输出 CSV 路径
        fns: List[Callable] = [],    # 论文处理函数
        groups: List[str] = ['conference'],
        only_accepted: bool = True   # 只获取已接受论文
    )
    
    def add_filter(self, filter_fn): ...  # 添加过滤器
    def scrape(self): ...                 # 执行抓取
```

### Extractor 类
```python
class Extractor:
    def __init__(
        self,
        fields: List[str],           # 顶层字段 ['forum', 'id']
        subfields: Dict[str, List],  # 嵌套字段 {'content': ['title', 'abstract']}
        include_subfield: bool = False
    )
```

### CSV 输出格式
| 字段 | 说明 |
|------|------|
| `id` | 唯一标识（会议名_序号）|
| `title` | 论文标题 |
| `keywords` | 关键词列表 |
| `abstract` | 摘要 |
| `pdf` | PDF 链接 |
| `forum` | 论文页面链接 |
| `year` | 年份 |
| `presentation_type` | 展示类型 (Oral/Spotlight/Poster) |

---

> **提示:** 项目采用模块化设计，不同数据来源独立实现，共享统一的输出格式。
