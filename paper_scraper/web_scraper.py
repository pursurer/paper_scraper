"""
ç½‘é¡µçˆ¬å–æ¨¡å—

ä»ä¼šè®®å®˜ç½‘ HTML é¡µé¢è·å–è®ºæ–‡å…ƒæ•°æ®ã€‚
æ”¯æŒ AAAIã€IJCAIã€ACLã€EMNLP ç­‰ä¼šè®®ã€‚
"""

import time
import random
import csv
import os
import re
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
from slugify import slugify

from .utils import to_csv


# ============ é€šç”¨å·¥å…·å‡½æ•° ============

# é»˜è®¤ User-Agent åˆ—è¡¨
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
    '(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 '
    '(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 '
    '(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
]


def get_random_user_agent() -> str:
    """è·å–éšæœº User-Agentã€‚"""
    return random.choice(USER_AGENTS)


def fetch_page(
    url: str,
    headers: Optional[Dict] = None,
    timeout: int = 30,
    retries: int = 3,
    delay: float = 1.0,
    verbose: bool = True
) -> Optional[str]:
    """
    è·å–ç½‘é¡µå†…å®¹ï¼Œå¸¦é‡è¯•æœºåˆ¶ã€‚
    
    Args:
        url: ç½‘é¡µ URL
        headers: è¯·æ±‚å¤´ï¼ˆå¯é€‰ï¼‰
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        retries: é‡è¯•æ¬¡æ•°
        delay: å¤±è´¥åçš„å»¶è¿Ÿï¼ˆç§’ï¼‰
        verbose: æ˜¯å¦æ‰“å°æ—¥å¿—
        
    Returns:
        ç½‘é¡µå†…å®¹ï¼ˆHTML å­—ç¬¦ä¸²ï¼‰ï¼Œå¤±è´¥è¿”å› None
    """
    if headers is None:
        headers = {
            'User-Agent': get_random_user_agent(),
        }
    
    for attempt in range(retries):
        try:
            response = requests.get(url, headers=headers, timeout=timeout)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            if verbose:
                print(f"   âš ï¸  è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{retries}): {e}")
            if attempt < retries - 1:
                time.sleep(delay * (attempt + 1))
    
    return None


def random_delay(min_sec: float = 2.0, max_sec: float = 5.0) -> None:
    """éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«ã€‚"""
    delay = random.uniform(min_sec, max_sec)
    time.sleep(delay)


# ============ IJCAI çˆ¬è™« ============

def scrape_ijcai(
    year: int,
    output_path: Optional[str] = None,
    verbose: bool = True
) -> List[Dict[str, Any]]:
    """
    çˆ¬å– IJCAI è®ºæ–‡åˆ—è¡¨ã€‚
    
    Args:
        year: ä¼šè®®å¹´ä»½ï¼ˆå¦‚ 2024ï¼‰
        output_path: è¾“å‡º CSV è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        verbose: æ˜¯å¦æ‰“å°æ—¥å¿—
        
    Returns:
        è®ºæ–‡åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« title, pdf_url, group, year, conference
        
    Example:
        >>> papers = scrape_ijcai(2024, output_path='ijcai_2024.csv')
        >>> len(papers)
        850
    """
    if verbose:
        print(f"\nğŸ” çˆ¬å– IJCAI {year} è®ºæ–‡...")
    
    # IJCAI proceedings URL
    if year >= 2017:
        base_url = f'https://www.ijcai.org/proceedings/{year}/'
    elif year >= 2003:
        base_url = f'https://www.ijcai.org/Proceedings/{year}/'
    else:
        if verbose:
            print(f"   âŒ ä¸æ”¯æŒ {year} å¹´ä¹‹å‰çš„ IJCAI")
        return []
    
    headers = {
        'User-Agent': get_random_user_agent(),
        'Referer': 'https://www.ijcai.org',
    }
    
    html = fetch_page(base_url, headers=headers, verbose=verbose)
    if not html:
        if verbose:
            print(f"   âŒ æ— æ³•è·å– IJCAI {year} é¡µé¢")
        return []
    
    papers = _parse_ijcai_page(html, base_url, year, verbose)
    
    if verbose:
        print(f"   âœ… æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
    
    # ä¿å­˜ CSV
    if output_path and papers:
        _save_papers_csv(papers, output_path, verbose)
    
    return papers


def _parse_ijcai_page(
    html: str,
    base_url: str,
    year: int,
    verbose: bool = True
) -> List[Dict[str, Any]]:
    """è§£æ IJCAI é¡µé¢ï¼Œæå–è®ºæ–‡ä¿¡æ¯ã€‚"""
    soup = BeautifulSoup(html, 'html.parser')
    papers = []
    
    if year >= 2017:
        # 2017+ ç»“æ„ï¼šsection_title -> paper_wrapper
        sections = soup.find_all('div', {'class': 'section_title'})
        
        for section in sections:
            group = slugify(section.get_text(strip=True))
            
            # æ‰¾åˆ°åŒçº§çš„è®ºæ–‡
            parent = section.parent
            if not parent:
                continue
            
            paper_wrappers = parent.find_all('div', {'class': 'paper_wrapper'})
            
            for wrapper in paper_wrappers:
                try:
                    # æ ‡é¢˜
                    title_div = wrapper.find('div', {'class': 'title'})
                    if not title_div:
                        continue
                    title = title_div.get_text(strip=True)
                    
                    # PDF é“¾æ¥
                    pdf_url = None
                    details = wrapper.find('div', {'class': 'details'})
                    if details:
                        for a in details.find_all('a'):
                            if 'PDF' in a.get_text():
                                pdf_url = urljoin(base_url, a.get('href', ''))
                                break
                    
                    papers.append({
                        'title': title,
                        'pdf_url': pdf_url or '',
                        'group': group,
                        'year': str(year),
                        'conference': 'IJCAI',
                    })
                except Exception as e:
                    if verbose:
                        print(f"   âš ï¸  è§£æè®ºæ–‡å¤±è´¥: {e}")
    else:
        # æ—§ç‰ˆç»“æ„ï¼Œç®€åŒ–å¤„ç†
        for a in soup.find_all('a', href=True):
            href = a.get('href', '')
            if href.endswith('.pdf'):
                title = a.get_text(strip=True)
                if title:
                    papers.append({
                        'title': title,
                        'pdf_url': urljoin(base_url, href),
                        'group': '',
                        'year': str(year),
                        'conference': 'IJCAI',
                    })
    
    return papers


# ============ AAAI çˆ¬è™« ============

def scrape_aaai(
    year: int,
    output_path: Optional[str] = None,
    verbose: bool = True
) -> List[Dict[str, Any]]:
    """
    çˆ¬å– AAAI è®ºæ–‡åˆ—è¡¨ã€‚
    
    Args:
        year: ä¼šè®®å¹´ä»½ï¼ˆå¦‚ 2024, 2025ï¼‰
        output_path: è¾“å‡º CSV è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        verbose: æ˜¯å¦æ‰“å°æ—¥å¿—
        
    Returns:
        è®ºæ–‡åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« title, pdf_url, group, year, conference
        
    Example:
        >>> papers = scrape_aaai(2025, output_path='aaai_2025.csv')
    """
    if verbose:
        print(f"\nğŸ” çˆ¬å– AAAI {year} è®ºæ–‡...")
    
    # è·å– track URLs
    track_urls = _get_aaai_track_urls(year, verbose)
    if not track_urls:
        if verbose:
            print(f"   âŒ æ— æ³•è·å– AAAI {year} tracks")
        return []
    
    all_papers = []
    
    for idx, (track_name, track_url) in enumerate(track_urls.items()):
        if verbose:
            print(f"\n   ğŸ“ [{idx+1}/{len(track_urls)}] {track_name}")
        
        papers = _scrape_aaai_track(track_url, year, verbose)
        all_papers.extend(papers)
        
        if verbose:
            print(f"      æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        
        # éšæœºå»¶è¿Ÿ
        if idx < len(track_urls) - 1:
            random_delay(3, 7)
    
    if verbose:
        print(f"\n   âœ… æ€»è®¡ {len(all_papers)} ç¯‡è®ºæ–‡")
    
    # ä¿å­˜ CSV
    if output_path and all_papers:
        _save_papers_csv(all_papers, output_path, verbose)
    
    return all_papers


def _get_aaai_track_urls(year: int, verbose: bool = True) -> Dict[str, str]:
    """è·å– AAAI å„ track çš„ URLã€‚"""
    track_urls = {}
    
    if year >= 2023:
        # æ–°ç‰ˆï¼šojs.aaai.org
        base_url = 'https://ojs.aaai.org/index.php/AAAI/issue/archive'
        headers = {
            'User-Agent': get_random_user_agent(),
            'Referer': 'https://ojs.aaai.org',
        }
        
        html = fetch_page(base_url, headers=headers, verbose=verbose)
        if not html:
            return {}
        
        soup = BeautifulSoup(html, 'html.parser')
        issues = soup.find('ul', {'class': 'issues_archive'})
        if not issues:
            return {}
        
        for li in issues.find_all('li'):
            h2 = li.find('h2')
            if not h2 or not h2.find('a'):
                continue
            
            track_name = slugify(h2.get_text(strip=True))
            # æ£€æŸ¥æ˜¯å¦æ˜¯æŒ‡å®šå¹´ä»½
            year_short = str(year - 2000)
            if f'aaai-{year_short}' in track_name.lower():
                track_url = h2.find('a').get('href', '')
                if track_url:
                    track_urls[track_name] = track_url
    else:
        # æ—§ç‰ˆï¼šaaai.org
        proceeding_th = year - 1986 if year >= 2010 else year - 1979
        base_url = f'https://aaai.org/proceeding/aaai-{proceeding_th:02d}-{year}/'
        
        headers = {
            'User-Agent': get_random_user_agent(),
            'Referer': 'https://aaai.org',
        }
        
        html = fetch_page(base_url, headers=headers, verbose=verbose)
        if not html:
            return {}
        
        soup = BeautifulSoup(html, 'html.parser')
        main = soup.find('main', {'class': 'content'})
        if not main:
            return {}
        
        for li in main.find_all('li'):
            a = li.find('a')
            if a:
                track_name = slugify(a.get_text(strip=True))
                track_url = a.get('href', '')
                if track_url:
                    track_urls[track_name] = track_url
    
    return track_urls


def _scrape_aaai_track(
    track_url: str,
    year: int,
    verbose: bool = True
) -> List[Dict[str, Any]]:
    """çˆ¬å–å•ä¸ª AAAI track çš„è®ºæ–‡ã€‚"""
    papers = []
    
    headers = {
        'User-Agent': get_random_user_agent(),
    }
    
    html = fetch_page(track_url, headers=headers, verbose=verbose)
    if not html:
        return []
    
    soup = BeautifulSoup(html, 'html.parser')
    
    if year >= 2023:
        # ojs.aaai.org ç»“æ„
        sections = soup.find_all('div', {'class': 'section'})
        
        for section in sections:
            h2 = section.find('h2')
            group = slugify(h2.get_text(strip=True)) if h2 else ''
            
            for li in section.find_all('li'):
                try:
                    h3 = li.find('h3', {'class': 'title'})
                    if not h3:
                        continue
                    title = h3.get_text(strip=True)
                    
                    pdf_link = li.find('a', {'class': 'obj_galley_link'})
                    pdf_url = ''
                    if pdf_link:
                        pdf_url = pdf_link.get('href', '').replace('view', 'download')
                    
                    papers.append({
                        'title': title,
                        'pdf_url': pdf_url,
                        'group': group,
                        'year': str(year),
                        'conference': 'AAAI',
                    })
                except Exception:
                    pass
    else:
        # aaai.org ç»“æ„
        tracks = soup.find_all('div', {'class': 'track-wrap'})
        
        for track in tracks:
            h2 = track.find('h2')
            group = slugify(h2.get_text(strip=True)) if h2 else ''
            
            for li in track.find_all('li'):
                try:
                    h5 = li.find('h5')
                    if not h5:
                        continue
                    title = h5.get_text(strip=True)
                    
                    pdf_link = li.find('a', {'class': 'wp-block-button'})
                    pdf_url = pdf_link.get('href', '') if pdf_link else ''
                    
                    papers.append({
                        'title': title,
                        'pdf_url': pdf_url,
                        'group': group,
                        'year': str(year),
                        'conference': 'AAAI',
                    })
                except Exception:
                    pass
    
    return papers


# ============ AISTATS çˆ¬è™« (PMLR) ============

# AISTATS å¹´ä»½åˆ° PMLR volume çš„æ˜ å°„
AISTATS_VOLUMES = {
    2025: 258, 2024: 238, 2023: 206, 2022: 151, 2021: 130,
    2020: 108, 2019: 89, 2018: 84, 2017: 54, 2016: 51,
    2015: 38, 2014: 33, 2013: 31, 2012: 22, 2011: 15,
    2010: 9, 2009: 5, 2007: 2,
}


def scrape_aistats(
    year: int,
    output_path: Optional[str] = None,
    verbose: bool = True
) -> List[Dict[str, Any]]:
    """
    çˆ¬å– AISTATS è®ºæ–‡åˆ—è¡¨ï¼ˆä» PMLRï¼‰ã€‚
    
    Args:
        year: ä¼šè®®å¹´ä»½ï¼ˆå¦‚ 2024ï¼‰
        output_path: è¾“å‡º CSV è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        verbose: æ˜¯å¦æ‰“å°æ—¥å¿—
        
    Returns:
        è®ºæ–‡åˆ—è¡¨
        
    Example:
        >>> papers = scrape_aistats(2024, output_path='aistats_2024.csv')
    """
    if verbose:
        print(f"\nğŸ” çˆ¬å– AISTATS {year} è®ºæ–‡ (PMLR)...")
    
    if year not in AISTATS_VOLUMES:
        if verbose:
            print(f"   âŒ ä¸æ”¯æŒ AISTATS {year}")
        return []
    
    volume = AISTATS_VOLUMES[year]
    papers = scrape_pmlr(f'v{volume}', 'AISTATS', year, verbose)
    
    if verbose:
        print(f"   âœ… æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
    
    if output_path and papers:
        _save_papers_csv(papers, output_path, verbose)
    
    return papers


def scrape_pmlr(
    volume: str,
    conference: str,
    year: int,
    verbose: bool = True
) -> List[Dict[str, Any]]:
    """
    ä» PMLR (Proceedings of Machine Learning Research) çˆ¬å–è®ºæ–‡ã€‚
    
    Args:
        volume: PMLR volumeï¼Œå¦‚ 'v238'
        conference: ä¼šè®®åç§°
        year: å¹´ä»½
        verbose: æ˜¯å¦æ‰“å°æ—¥å¿—
        
    Returns:
        è®ºæ–‡åˆ—è¡¨
    """
    base_url = f'https://proceedings.mlr.press/{volume}/'
    
    headers = {
        'User-Agent': get_random_user_agent(),
    }
    
    html = fetch_page(base_url, headers=headers, verbose=verbose)
    if not html:
        return []
    
    soup = BeautifulSoup(html, 'html.parser')
    paper_divs = soup.find_all('div', {'class': 'paper'})
    
    papers = []
    for div in paper_divs:
        try:
            # æ ‡é¢˜
            title_p = div.find('p', {'class': 'title'})
            if not title_p:
                continue
            title = title_p.get_text(strip=True)
            
            # PDF é“¾æ¥
            pdf_url = ''
            links_p = div.find('p', {'class': 'links'})
            if links_p:
                for a in links_p.find_all('a'):
                    text = a.get_text(strip=True).lower()
                    if 'pdf' in text or 'download' in text:
                        pdf_url = a.get('href', '')
                        break
            
            papers.append({
                'title': title,
                'pdf_url': pdf_url,
                'group': '',
                'year': str(year),
                'conference': conference,
            })
        except Exception:
            pass
    
    return papers


# ============ ACL Anthology çˆ¬è™« ============

def scrape_acl_anthology(
    conference: str,
    year: int,
    output_path: Optional[str] = None,
    verbose: bool = True
) -> List[Dict[str, Any]]:
    """
    ä» ACL Anthology çˆ¬å–è®ºæ–‡åˆ—è¡¨ã€‚
    
    æ”¯æŒ ACL, EMNLP, NAACL, EACL, COLING ç­‰ä¼šè®®ã€‚
    
    Args:
        conference: ä¼šè®®åç§° ('ACL', 'EMNLP', 'NAACL' ç­‰)
        year: ä¼šè®®å¹´ä»½
        output_path: è¾“å‡º CSV è·¯å¾„
        verbose: æ˜¯å¦æ‰“å°æ—¥å¿—
        
    Returns:
        è®ºæ–‡åˆ—è¡¨
        
    Example:
        >>> papers = scrape_acl_anthology('ACL', 2023)
    """
    if verbose:
        print(f"\nğŸ” çˆ¬å– {conference} {year} è®ºæ–‡ (ACL Anthology)...")
    
    # ACL Anthology çš„ä¼šè®®ä»£ç æ˜ å°„
    conf_codes = {
        'ACL': 'acl',
        'EMNLP': 'emnlp',
        'NAACL': 'naacl',
        'EACL': 'eacl',
        'COLING': 'coling',
        'FINDINGS': 'findings',
    }
    
    conf_upper = conference.upper()
    if conf_upper not in conf_codes:
        if verbose:
            print(f"   âŒ ä¸æ”¯æŒçš„ä¼šè®®: {conference}")
        return []
    
    code = conf_codes[conf_upper]
    
    # ACL Anthology URL æ ¼å¼
    # ä¸»ä¼šè®®: https://aclanthology.org/events/acl-2023/
    base_url = f'https://aclanthology.org/events/{code}-{year}/'
    
    headers = {
        'User-Agent': get_random_user_agent(),
    }
    
    html = fetch_page(base_url, headers=headers, verbose=verbose)
    if not html:
        if verbose:
            print(f"   âŒ æ— æ³•è·å– {conference} {year} é¡µé¢")
        return []
    
    papers = _parse_acl_anthology_page(html, conf_upper, year, verbose)
    
    if verbose:
        print(f"   âœ… æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
    
    if output_path and papers:
        _save_papers_csv(papers, output_path, verbose)
    
    return papers


def _parse_acl_anthology_page(
    html: str,
    conference: str,
    year: int,
    verbose: bool = True
) -> List[Dict[str, Any]]:
    """è§£æ ACL Anthology é¡µé¢ã€‚"""
    soup = BeautifulSoup(html, 'html.parser')
    papers = []
    
    # æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡æ¡ç›®
    # ACL Anthology ä½¿ç”¨ <p class="d-sm-flex align-items-stretch"> åŒ…è£…è®ºæ–‡
    paper_entries = soup.find_all('p', {'class': 'd-sm-flex'})
    
    for entry in paper_entries:
        try:
            # æŸ¥æ‰¾æ ‡é¢˜é“¾æ¥
            title_span = entry.find('span', {'class': 'd-block'})
            if not title_span:
                continue
            
            title_link = title_span.find('a', {'class': 'align-middle'})
            if not title_link:
                continue
            
            title = title_link.get_text(strip=True)
            paper_url = title_link.get('href', '')
            
            # PDF é“¾æ¥é€šå¸¸æ˜¯ paper_url + .pdf
            pdf_url = ''
            if paper_url:
                # ä»è®ºæ–‡é¡µé¢ URL æ„é€  PDF URL
                # https://aclanthology.org/2023.acl-long.1/ -> https://aclanthology.org/2023.acl-long.1.pdf
                pdf_url = f'https://aclanthology.org{paper_url}'.rstrip('/') + '.pdf'
            
            papers.append({
                'title': title,
                'pdf_url': pdf_url,
                'group': '',
                'year': str(year),
                'conference': conference,
            })
        except Exception:
            pass
    
    return papers


def scrape_acl(
    year: int,
    output_path: Optional[str] = None,
    verbose: bool = True
) -> List[Dict[str, Any]]:
    """çˆ¬å– ACL è®ºæ–‡ã€‚"""
    return scrape_acl_anthology('ACL', year, output_path, verbose)


def scrape_emnlp(
    year: int,
    output_path: Optional[str] = None,
    verbose: bool = True
) -> List[Dict[str, Any]]:
    """çˆ¬å– EMNLP è®ºæ–‡ã€‚"""
    return scrape_acl_anthology('EMNLP', year, output_path, verbose)


def scrape_naacl(
    year: int,
    output_path: Optional[str] = None,
    verbose: bool = True
) -> List[Dict[str, Any]]:
    """çˆ¬å– NAACL è®ºæ–‡ã€‚"""
    return scrape_acl_anthology('NAACL', year, output_path, verbose)


# ============ é€šç”¨ä¿å­˜å‡½æ•° ============

def _save_papers_csv(
    papers: List[Dict[str, Any]],
    output_path: str,
    verbose: bool = True
) -> None:
    """ä¿å­˜è®ºæ–‡åˆ—è¡¨åˆ° CSVã€‚"""
    if not papers:
        return
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)
    
    # è½¬æ¢æ ¼å¼ä»¥é€‚é… to_csv
    papers_for_csv = []
    for idx, p in enumerate(papers):
        papers_for_csv.append({
            'id': f"{p.get('conference', 'CONF')}_{p.get('year', '')}_{idx+1:04d}",
            'title': p.get('title', ''),
            'pdf': p.get('pdf_url', ''),
            'group': p.get('group', ''),
            'year': p.get('year', ''),
            'conference': p.get('conference', ''),
            'keywords': '',
            'abstract': '',
        })
    
    to_csv(papers_for_csv, output_path)
    
    if verbose:
        print(f"   ğŸ’¾ å·²ä¿å­˜åˆ° {output_path}")


# ============ ç»Ÿä¸€å…¥å£ ============

def scrape_conference(
    conference: str,
    year: int,
    output_path: Optional[str] = None,
    verbose: bool = True
) -> List[Dict[str, Any]]:
    """
    ç»Ÿä¸€çš„ä¼šè®®çˆ¬å–å…¥å£ã€‚
    
    Args:
        conference: ä¼šè®®åç§°ï¼ˆ'AAAI', 'IJCAI' ç­‰ï¼‰
        year: ä¼šè®®å¹´ä»½
        output_path: è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰
        verbose: æ˜¯å¦æ‰“å°æ—¥å¿—
        
    Returns:
        è®ºæ–‡åˆ—è¡¨
        
    Example:
        >>> papers = scrape_conference('IJCAI', 2024)
    """
    conference = conference.upper()
    
    scrapers = {
        'IJCAI': scrape_ijcai,
        'AAAI': scrape_aaai,
        'AISTATS': scrape_aistats,
        'ACL': scrape_acl,
        'EMNLP': scrape_emnlp,
        'NAACL': scrape_naacl,
    }
    
    if conference not in scrapers:
        supported = ', '.join(sorted(scrapers.keys()))
        raise ValueError(f"ä¸æ”¯æŒçš„ä¼šè®®: {conference}ã€‚æ”¯æŒ: {supported}")
    
    return scrapers[conference](year, output_path, verbose)


# ============ æ‰¹é‡çˆ¬å– ============

def batch_scrape(
    conferences: List[str],
    years: List[int],
    output_dir: str = './output',
    verbose: bool = True
) -> Dict[str, List[Dict[str, Any]]]:
    """
    æ‰¹é‡çˆ¬å–å¤šä¸ªä¼šè®®ã€‚
    
    Args:
        conferences: ä¼šè®®åç§°åˆ—è¡¨
        years: å¹´ä»½åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        verbose: æ˜¯å¦æ‰“å°æ—¥å¿—
        
    Returns:
        {ä¼šè®®_å¹´ä»½: è®ºæ–‡åˆ—è¡¨} å­—å…¸
        
    Example:
        >>> results = batch_scrape(['IJCAI', 'AAAI'], [2023, 2024])
    """
    os.makedirs(output_dir, exist_ok=True)
    results = {}
    
    for conf in conferences:
        for year in years:
            key = f"{conf}_{year}"
            output_path = os.path.join(output_dir, f"{key}.csv")
            
            if verbose:
                print(f"\n{'='*50}")
                print(f"ğŸ“š çˆ¬å– {conf} {year}")
                print(f"{'='*50}")
            
            try:
                papers = scrape_conference(conf, year, output_path, verbose)
                results[key] = papers
            except Exception as e:
                if verbose:
                    print(f"   âŒ çˆ¬å–å¤±è´¥: {e}")
                results[key] = []
            
            # ä¼šè®®é—´å»¶è¿Ÿ
            random_delay(5, 10)
    
    return results

