"""
Paper Scraper CLI å…¥å£

æ”¯æŒé€šè¿‡ python -m paper_scraper è°ƒç”¨ã€‚

Usage:
    # OpenReview æ¥æº
    python -m paper_scraper --conference ICLR --year 2024 --output iclr_2024.csv
    
    # ç½‘é¡µçˆ¬å–æ¥æº
    python -m paper_scraper --conference IJCAI --year 2024 --output ijcai_2024.csv
    
    # PDF æå–
    python -m paper_scraper --pdf-dir ./aamas2025 --year 2025 --output aamas_2025.csv
    
    # æ‰¹é‡çˆ¬å–
    python -m paper_scraper --conferences ICLR ICML --years 2023 2024 --output-dir ./output
"""

import argparse
import sys
import os

from . import (
    __version__,
    SOURCES,
    Scraper,
    Extractor,
    scrape_conference,
    batch_scrape,
    extract_aamas_metadata,
    is_pdf_available,
)


def get_source_type(conference: str) -> str:
    """è·å–ä¼šè®®çš„æ•°æ®æºç±»å‹ã€‚"""
    conf_upper = conference.upper()
    for source_type, conferences in SOURCES.items():
        # å¤§å°å†™ä¸æ•æ„Ÿæ¯”è¾ƒ
        if conf_upper in [c.upper() for c in conferences]:
            return source_type
    return 'unknown'


def create_parser() -> argparse.ArgumentParser:
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨ã€‚"""
    parser = argparse.ArgumentParser(
        prog='paper_scraper',
        description='é¡¶ä¼šè®ºæ–‡è·å–å·¥å…· - æ”¯æŒ OpenReviewã€ç½‘é¡µçˆ¬å–ã€PDF æå–',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # çˆ¬å– ICLR 2024 (OpenReview)
  python -m paper_scraper -c ICLR -y 2024 -o iclr_2024.csv

  # çˆ¬å– IJCAI 2024 (ç½‘é¡µçˆ¬å–)
  python -m paper_scraper -c IJCAI -y 2024 -o ijcai_2024.csv

  # æ‰¹é‡çˆ¬å–å¤šä¸ªä¼šè®®
  python -m paper_scraper -c ICLR ICML -y 2023 2024 --output-dir ./output

  # ä» PDF æå– AAMAS å…ƒæ•°æ®
  python -m paper_scraper --pdf-dir ./aamas2025 -y 2025 -o aamas_2025.csv

  # å¸¦å…³é”®è¯è¿‡æ»¤
  python -m paper_scraper -c ICLR -y 2024 -k "reinforcement learning" -o rl_papers.csv

æ”¯æŒçš„ä¼šè®®:
  OpenReview: ICLR, ICML, NeurIPS
  ç½‘é¡µçˆ¬å–: AAAI, IJCAI, ACL, EMNLP, NAACL, AISTATS
  PDF æå–: AAMAS
        """
    )
    
    # ç‰ˆæœ¬ä¿¡æ¯
    parser.add_argument(
        '-v', '--version',
        action='version',
        version=f'paper_scraper {__version__}'
    )
    
    # ä¼šè®®é€‰é¡¹
    parser.add_argument(
        '-c', '--conference', '--conferences',
        nargs='+',
        dest='conferences',
        help='ä¼šè®®åç§°ï¼ˆå¯æŒ‡å®šå¤šä¸ªï¼‰'
    )
    
    # å¹´ä»½é€‰é¡¹
    parser.add_argument(
        '-y', '--year', '--years',
        nargs='+',
        dest='years',
        help='å¹´ä»½ï¼ˆå¯æŒ‡å®šå¤šä¸ªï¼‰'
    )
    
    # è¾“å‡ºé€‰é¡¹
    parser.add_argument(
        '-o', '--output',
        help='è¾“å‡º CSV æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--output-dir',
        help='æ‰¹é‡è¾“å‡ºç›®å½•ï¼ˆä¸ --output äº’æ–¥ï¼‰'
    )
    
    # å…³é”®è¯è¿‡æ»¤
    parser.add_argument(
        '-k', '--keywords',
        nargs='+',
        default=[],
        help='è¿‡æ»¤å…³é”®è¯ï¼ˆå¯æŒ‡å®šå¤šä¸ªï¼‰'
    )
    
    # PDF æå–é€‰é¡¹
    parser.add_argument(
        '--pdf-dir',
        help='PDF ç›®å½•è·¯å¾„ï¼ˆç”¨äº AAMAS ç­‰ PDF æå–ï¼‰'
    )
    
    # å…¶ä»–é€‰é¡¹
    parser.add_argument(
        '--quiet', '-q',
        action='store_true',
        help='å®‰é™æ¨¡å¼ï¼Œå‡å°‘è¾“å‡º'
    )
    
    parser.add_argument(
        '--list-conferences',
        action='store_true',
        help='åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„ä¼šè®®'
    )
    
    return parser


def list_conferences() -> None:
    """åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„ä¼šè®®ã€‚"""
    print("\nğŸ“š æ”¯æŒçš„ä¼šè®®åˆ—è¡¨:\n")
    
    print("ğŸ”— OpenReview API:")
    for conf in SOURCES['openreview']:
        print(f"   - {conf}")
    
    print("\nğŸŒ ç½‘é¡µçˆ¬å–:")
    for conf in SOURCES['web_scrape']:
        print(f"   - {conf}")
    
    print("\nğŸ“„ PDF æå–:")
    for conf in SOURCES['pdf_extract']:
        print(f"   - {conf}")
    
    print()


def run_openreview_scrape(
    conferences: list,
    years: list,
    keywords: list,
    output: str,
    verbose: bool
) -> int:
    """è¿è¡Œ OpenReview çˆ¬å–ã€‚"""
    if verbose:
        print(f"\nğŸ” OpenReview çˆ¬å–: {', '.join(conferences)} ({', '.join(years)})")
    
    try:
        extractor = Extractor(
            fields=['forum'],
            subfields={'content': ['title', 'keywords', 'abstract', 'pdf']}
        )
        
        # æ·»åŠ è‡ªå®šä¹‰å¤„ç†å‡½æ•°
        def modify_paper(paper):
            paper.forum = f"https://openreview.net/forum?id={paper.forum}"
            if 'pdf' in paper.content:
                pdf_value = paper.content['pdf']
                # å¤„ç† OpenReview API v2 çš„ {'value': '...'} æ ¼å¼
                if isinstance(pdf_value, dict) and 'value' in pdf_value:
                    pdf_value = pdf_value['value']
                paper.content['pdf'] = f"https://openreview.net{pdf_value}"
            return paper
        
        scraper = Scraper(
            conferences=conferences,
            years=years,
            keywords=keywords,
            extractor=extractor,
            fpath=output,
            fns=[modify_paper]
        )
        
        if keywords:
            from .filters import title_filter, abstract_filter, keywords_filter
            scraper.add_filter(title_filter)
            scraper.add_filter(abstract_filter)
            scraper.add_filter(keywords_filter)
        
        scraper()
        return 0
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return 1


def run_web_scrape(
    conferences: list,
    years: list,
    output: str,
    output_dir: str,
    verbose: bool
) -> int:
    """è¿è¡Œç½‘é¡µçˆ¬å–ã€‚"""
    try:
        if len(conferences) == 1 and len(years) == 1 and output:
            # å•ä¼šè®®å•å¹´ä»½
            papers = scrape_conference(
                conferences[0],
                int(years[0]),
                output,
                verbose
            )
            if verbose:
                print(f"\nâœ… å®Œæˆ! å…± {len(papers)} ç¯‡è®ºæ–‡")
        else:
            # æ‰¹é‡çˆ¬å–
            out_dir = output_dir or './output'
            results = batch_scrape(
                conferences,
                [int(y) for y in years],
                out_dir,
                verbose
            )
            if verbose:
                total = sum(len(papers) for papers in results.values())
                print(f"\nâœ… å®Œæˆ! å…± {total} ç¯‡è®ºæ–‡")
        
        return 0
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return 1


def run_pdf_extract(
    pdf_dir: str,
    year: str,
    output: str,
    verbose: bool
) -> int:
    """è¿è¡Œ PDF æå–ã€‚"""
    if not is_pdf_available():
        print("âŒ æœªå®‰è£… PDF åº“ï¼Œè¯·å®‰è£…: pip install PyMuPDF")
        return 1
    
    try:
        papers = extract_aamas_metadata(
            pdf_dir,
            int(year),
            output,
            verbose
        )
        
        if verbose:
            print(f"\nâœ… å®Œæˆ! å…± {len(papers)} ç¯‡è®ºæ–‡")
        
        return 0
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return 1


def main(args=None) -> int:
    """ä¸»å…¥å£å‡½æ•°ã€‚"""
    parser = create_parser()
    parsed = parser.parse_args(args)
    
    verbose = not parsed.quiet
    
    # åˆ—å‡ºä¼šè®®
    if parsed.list_conferences:
        list_conferences()
        return 0
    
    # PDF æå–æ¨¡å¼
    if parsed.pdf_dir:
        if not parsed.years or len(parsed.years) != 1:
            print("âŒ PDF æå–æ¨¡å¼éœ€è¦æŒ‡å®šå•ä¸ªå¹´ä»½ (-y)")
            return 1
        if not parsed.output:
            print("âŒ éœ€è¦æŒ‡å®šè¾“å‡ºæ–‡ä»¶ (-o)")
            return 1
        
        return run_pdf_extract(
            parsed.pdf_dir,
            parsed.years[0],
            parsed.output,
            verbose
        )
    
    # å¸¸è§„çˆ¬å–æ¨¡å¼
    if not parsed.conferences:
        print("âŒ éœ€è¦æŒ‡å®šä¼šè®® (-c)")
        parser.print_help()
        return 1
    
    if not parsed.years:
        print("âŒ éœ€è¦æŒ‡å®šå¹´ä»½ (-y)")
        return 1
    
    if not parsed.output and not parsed.output_dir:
        print("âŒ éœ€è¦æŒ‡å®šè¾“å‡ºæ–‡ä»¶ (-o) æˆ–è¾“å‡ºç›®å½• (--output-dir)")
        return 1
    
    # åˆ¤æ–­æ•°æ®æºç±»å‹
    source_types = set(get_source_type(c) for c in parsed.conferences)
    
    if 'unknown' in source_types:
        unknown_confs = [c for c in parsed.conferences if get_source_type(c) == 'unknown']
        print(f"âŒ ä¸æ”¯æŒçš„ä¼šè®®: {', '.join(unknown_confs)}")
        list_conferences()
        return 1
    
    if len(source_types) > 1:
        print("âš ï¸  æ··åˆæ•°æ®æºï¼Œå°†åˆ†åˆ«å¤„ç†...")
    
    # OpenReview æ¥æº
    openreview_confs = [c for c in parsed.conferences if get_source_type(c) == 'openreview']
    if openreview_confs:
        output = parsed.output or os.path.join(parsed.output_dir, 'openreview_papers.csv')
        result = run_openreview_scrape(
            openreview_confs,
            parsed.years,
            parsed.keywords,
            output,
            verbose
        )
        if result != 0:
            return result
    
    # ç½‘é¡µçˆ¬å–æ¥æº
    web_confs = [c for c in parsed.conferences if get_source_type(c) == 'web_scrape']
    if web_confs:
        result = run_web_scrape(
            web_confs,
            parsed.years,
            parsed.output if len(web_confs) == 1 and len(parsed.years) == 1 else None,
            parsed.output_dir,
            verbose
        )
        if result != 0:
            return result
    
    return 0


if __name__ == '__main__':
    sys.exit(main())

